{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47197e6d-2f22-4338-b387-619cef5dd5bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Description:\n",
    "Transform weather-data in Bronze layer to convert complex data type values in source into tabular format and load it into Silver Layer . Store the final output as delta lake table in silver scheme.\n",
    "\n",
    "Source File Path : \"abfss://bronze@datalakestorageaccountname.dfs.core.windows.net/weather-data/\n",
    "\n",
    "##### Input Columns:\n",
    "daily:struct ,\n",
    "daily_units:struct ,\n",
    "latitude:double,\n",
    "longitude:double,\n",
    "marketName:string\n",
    "\n",
    "##### Output Columns \n",
    "marketName ,weatherDate,unitOfTemparature,maximumTemparature ,minimumTemparature,unitOfRainFall,rainFall,latitude,longitude\n",
    "\n",
    "##### Output table name : weather_data_silver\n",
    "\n",
    "##### Transformation Rules:\n",
    "Extract WeatherDate values from the source column daily.time\n",
    "\n",
    "Extract maximumTemparature values from source column daily.temperature_2m_max\n",
    "\n",
    "Extract  minimumTemparature values from source column daily.temperature_2m_min\n",
    "\n",
    "Extract rainFall values from source column daily.rain_sum\n",
    "\n",
    "Extract unitOfTemparature and unitOfRainFall values from source columns daily_units.temperature_2m_max and daily_units.rain_sum respectively\n",
    "\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html#pyspark.sql.DataFrame.join\" target=\"_blank\">**DataFrame Joins** </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ed42af-11ed-4baa-bb5d-2c786093e3f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 1: Define the Variables to read weather-data ingested in bronze Layer\n",
    "\n",
    "1. Replace <datalakestorageaccountname> with the ADLS account name crated in your account\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf48dc0-842a-424d-9f55-55608f6560b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7eec543-f187-4656-83c8-7a09d582cebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weatherDataSourceLayerName = 'bronze'\n",
    "weatherDataSourceStorageAccountName = 'madhupavanadls'\n",
    "weatherDataSourceFolderName = 'weather-data'\n",
    "\n",
    "weatherDataSourceFolderPath = f\"abfss://{weatherDataSourceLayerName}@{weatherDataSourceStorageAccountName}.dfs.core.windows.net/{weatherDataSourceFolderName}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da601d03-17fb-4bb6-846b-7cfb308e4bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 2: Create Spark Dataframe For weather-data in Json form stored in bronze layer\n",
    "\n",
    "1. Define Spark Dataframe variable name as weatherDataBronzeDF\n",
    "1. Use spark.read.json method to read the source data path defined above using the variable weatherDataSourceFolderPath \n",
    "1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb34fd2-e557-464b-b2ab-3cf39c459348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weatherDataBronzeDF = spark.read.format(\"delta\").load(weatherDataSourceFolderPath)\n",
    "weatherDataBronzeDF_main = weatherDataBronzeDF.filter(length(col('market_name')) > 0)\n",
    "original_df = weatherDataBronzeDF_main.checkpoint()\n",
    "\n",
    "\n",
    "first_split = original_df.select(col('market_name'),col('latitude'),col('longitude'),explode(col('daily.time')).alias('weather_date'),\n",
    "                                         col('daily_units.rain_sum').alias('unit_of_rainfall'),col('daily_units.temperature_2m_max').alias('unit_of_temperature')\n",
    "                                         ,monotonically_increasing_id().alias('sequenceId')).cache()\n",
    "\n",
    "                           \n",
    "                        #    col('daily.temperature_2m_max').alias('maximumTemparature'),col('daily.temperature_2m_min').alias('minimumTemparature'),col('daily.rain_sum').alias('rainfall'),col('daily_units.rain_sum').alias('unit_of_rainfall'),col('daily_units.temperature_2m_max').alias('unit_of_temperature')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4c1c48-b8a0-48d3-ab8e-ad51fcc4a64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temperature_2m_max_second_split =  original_df.select(explode(col('daily.temperature_2m_max')).alias('maximumTemparature'),monotonically_increasing_id().alias('temperature_2m_max_sequenceId')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "261ea7b6-458d-44c5-8a91-c57ddc9301f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temperature_2m_min_second_split =  original_df.select(explode(col('daily.temperature_2m_min')).alias('minimumTemparature'),monotonically_increasing_id().alias('temperature_2m_min_sequenceId')).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49108a22-e6c4-417e-8799-e22418fe8179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 3: Convert Weathe Date Values in ARRAY format to ROWS Using Explode\n",
    "\n",
    "1. Import all functions from pyspark.sql.functions package\n",
    "1. Define New Spark Dataframe variable name as weatherDataDailyDateTransDF\n",
    "1. Use Dataframe select method to select the columns given below from source Spark Dataframe variable weatherDataBronzeDF\n",
    "1. First select column is \"daily.time\" and apply the explode function on this source column and also add alias for exploded values column as \"weatherDate\"\n",
    "1. Along with above explode select the columns \"marketName\" , \"latitude\" , \"longitude\" from source Spark Dataframe\n",
    "1. Last column in the select is running sequence id generated by Spark function monotonically_increasing_id() and add alias name as 'sequenceId'\n",
    "1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e51afc-3eb3-4450-9345-d8d3af3f469c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temperature_2m_min_third_split =  original_df.select(explode(col('daily.rain_sum')).alias('rainfall'),monotonically_increasing_id().alias('rainfall_sequenceId')).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a30d148-36db-4782-8bad-22d0eb481e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir(\"/tmp/spark_checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4de4184-4d62-46fd-a274-dcb25e26bd85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = first_split.join(\n",
    "    temperature_2m_max_second_split,\n",
    "    first_split['sequenceId'] == temperature_2m_max_second_split['temperature_2m_max_sequenceId'],\n",
    "    'inner'\n",
    ").join(\n",
    "    temperature_2m_min_second_split,\n",
    "    first_split['sequenceId'] == temperature_2m_min_second_split['temperature_2m_min_sequenceId'],\n",
    "    'inner'\n",
    ").join(\n",
    "    temperature_2m_min_third_split,\n",
    "    first_split['sequenceId'] == temperature_2m_min_third_split['rainfall_sequenceId'],\n",
    "    'inner'\n",
    ").select(col('market_name'),col('latitude'),col('longitude'),col('weather_date'),col('unit_of_rainfall'),col('unit_of_temperature'),col('maximumTemparature'),col('minimumTemparature'),col('rainfall'))\n",
    "\n",
    "final_data = final_df.checkpoint()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd511d38-4248-4ae8-8a58-c8370cf1ffeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_data = final_data.withColumn('weather_date', to_date(col('weather_date'),'yyyy-MM-dd'))\n",
    "# DBTITLE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2170d81-92b5-4cdd-a52d-ef7900248cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_data.write.format('delta').mode('overwrite').saveAsTable('uc_prod.silver.weather_data_silver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768d22fa-6d9c-45cd-a8b5-a0e5a65785bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 4: Convert Maximum Temparature Values in ARRAY format to ROWS Using Explode\n",
    "\n",
    "1. Define New Spark Dataframe variable name as weatherDataMaxTemparatureTransDF\n",
    "1. Use Dataframe select method to select the columns given below from source Spark Dataframe variable weatherDataBronzeDF\n",
    "1. First select column is \"daily.temperature_2m_max\" and apply the explode function on this source column and also add alias for exploded values column as \"maximumTemparature\"\n",
    "1. Along with above explode select the columns \"marketName\" , \"latitude\" , \"longitude\" from source Spark Dataframe\n",
    "1. Last column in the select is running sequence id generated by Spark function monotonically_increasing_id() and add alias name as 'sequenceId'\n",
    "1. Add one more column from the Source Spark Dataframe \"daily_units.temperature_2m_max\" and provide alias name as \"unitOfTemparature\"\n",
    "1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7899958-0284-4f3f-a5c8-5ba85c59b357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 5: Convert Minimum Temparature Values in ARRAY format to ROWS Using Explode\n",
    "\n",
    "1. Define New Spark Dataframe variable name as weatherDataMinTemparatureTransDF\n",
    "1. Use Dataframe select method to select the columns given below from source Spark Dataframe variable weatherDataBronzeDF\n",
    "1. First select column is \"daily.temperature_2m_min\" and apply the explode function on this source column and also add alias for exploded values column as \"minimumTemparature\"\n",
    "1. Along with above explode select the columns \"marketName\" , \"latitude\" , \"longitude\" from source Spark Dataframe\n",
    "1. Last column in the select is running sequence id generated by Spark function monotonically_increasing_id() and add alias name as 'sequenceId'\n",
    "1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af1dac9-79a9-4bae-a3e4-a6891b09fa47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 6: Convert Rain Fall Values in ARRAY format to ROWS Using Explode\n",
    "\n",
    "1. Define New Spark Dataframe variable name as weatherDataRainFallTransDF\n",
    "1. Use Dataframe select method to select the columns given below from source Spark Dataframe variable weatherDataBronzeDF\n",
    "1. First select column is \"daily.rain_sum\" and apply the explode function on this source column and also add alias for exploded values column as \"rainFall\"\n",
    "1. Along with above explode select the columns \"marketName\" , \"latitude\" , \"longitude\" from source Spark Dataframe\n",
    "1. Last column in the select is running sequence id generated by Spark function monotonically_increasing_id() and add alias name as 'sequenceId'\n",
    "1. Add one more column from the Source Spark Dataframe \"daily_units.rain_sum\" and provide alias name as \"unitOfRainFall\"\n",
    "1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62419261-614c-447f-a961-fb9d6c67172d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 7: Join All Intermediate Dataframes To Merge All Data & Write Into Silver Layer\n",
    "\n",
    "1. Define New Spark Dataframe variable name as weatherDataTransDF\n",
    "1. Join weatherDataDailyDateTransDF with weatherDataMaxTemparatureTransDF Using the Joining Columns ['marketName','latitude','longitude','sequenceId']\n",
    "1. Extend weatherDataDailyDateTransDF with weatherDataMinTemparatureTransDF Using the Joining Columns ['marketName','latitude','longitude','sequenceId']\n",
    "1. Extend weatherDataDailyDateTransDF with weatherDataRainFallTransDF Using the Joining Columns ['marketName','latitude','longitude','sequenceId']\n",
    "1. Select the Columns \"marketName\" , \"weatherDate\" , \"unitOfTemparature\" , \"maximumTemparature\" , \"minimumTemparature\" , \"unitOfRainFall\" , \"rainFall\" , \"latitude\" and \"longitude\" to write final output columns into silve layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030cbf8f-23b5-452b-8c9f-65eefb19c432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 8: Write the Final Transformed Dataframe Into Silve Layer As Delta Table\n",
    "\n",
    "1. Write Final Spark Dataframe weatherDataTransDF values using spark.write method\n",
    "1. Use Write mode as overwrite \n",
    "1. Write the data into the Datalake Table \"pricing_analytics.silver.weather_data_silver\" using saveAsTable Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa4b6da-257c-4f84-9951-e5e2db78e315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 9: Test The Data Stored in Tranformed Silve Layer Table\n",
    "1. Write SELECT query to select the data from pricing_analytics.silver.weather_data_silver table\n",
    "1. Check the data for any one of the Market matches with the source data in Complex JSON format"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6116485229224501,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05-Transform-DataLake-WeatherData-Assignment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
